---
title: "structuring-ML-project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### WEEK 1 ####


### INTRODUCTION TO ML STRATEGY  ####


# WHY ML STRATEGY

When you have a model with good accuracy, what could be options to make it better?
- collect more data
- more diverse data set
- training for longer with gradient descent
- bigger/smaller network
- try dropout
- add L2 regularixation
- change network architecture (activation functions, hidden units, etc.)

what are ways of quickly testing which of these options will improve the results?

# ORTHOGONALIZATION

## TV tuning examples
each knob does only 1 thing to make sure thath the screen is centred correctly

## Car example

wheel for *stearing8, something else or breaking another for accellaring 

## chain of assumpotions in ML

1. fit training set well ==> 
2. fit dev set well ==> 
3. fit test set well ==> 
4. performs well in real world 

if 1. not very good -- try bigger network, Adam, etc.
if 2. not very good -- try regularization, bigger train set
if 3. not very good -- try bigger dev set
if 4. not very good -- try chaning dev set or cost function


### SETTING UP YOUR GOAL ####


# SINGLE NUMBER EVALUATION METRIC

**precision** =  of all the examples identified examples as positive , what % was actually positive?

**recall** = of all positive examples, how many are correctly recognised?

there's often a trade-off between the two, so which one to choose?

**F1 Score** = harmonic mean of presicion and recall

iterating improvement of ML model = have a well definied dev set and a single number evaluation metric, such as F1 score

if you have multi-label classification problem, take accuracy average actoss all the groups as a criterium for judging different algorithms


# SATISFICING AND OPTIMIZING METRIC

e.g. we may want to maximize accuracy subject to running time being <= 100ms
here, accuracy is an optimizing metric and running time satisficing metric

if you have N metrics you care about, pick 1 to be an optimising metric and the rest should be optimicing metrics with specified, 'good enough' threshold. 


# train/Dev/test distributions


## dev/test sets

dev = development set = hold out cross-validation set

dev and test sets should come from the same distributions and should be evaluated using the same metric!

# Size of the dev and test sets

## old way of splitting data

train/test 0.7/0.3 split
or
train/dev/test - 0.6/0.2/0.2 split

appropriate if you work with relatively small datasets

but if you work with relatively big data (e.g. 1M obs) it makes sense to train on 98% and use 1% for dev and test sets respectively


## size of test set
set you test set to be big enough to be big enough to give high confidence in the overall performance of your system

# when to change dev/test sets and metrics
example of 2 algorithms where one is more accurate but lets pornoghraphic images in while the other one does not. In such case, 'pure' accuracy measures are not appropriate so we need to redefine our new error metric so that it contains penalty for, e.g. occurance and the number of pornographic images.

ORTHOGONALIZATION OF THE PROBLEM
1) what's the success metric/target?
2) how to do well on this target?

So the guideline is, if doing well on your metric and your current dev sets or dev and test sets' distribution, if that does not correspond to doing well on the application you actually care about, then change your metric and your dev test set. 


# COMPARING TO HUMAN LEVEL PERFORMANCE --------------

# WHY HUMAN LEVEL PERFORMANCE?

**bayes optimal error** - best possible error
i.e. optimal level of accuracy may not be 100% - e.g. audio is so noisy or picture sp blurry that you can't classify them all correctly.

For tasks that humans are good at, so long as your machine learning algorithm is still worse than the human, you can 
+ get labeled data from humans. That is you can ask people, ask higher humans, to label examples for you so that you can have more data to feed your learning algorithm. (manual error analysis). 
+ ask people to look at examples that your algorithm's getting wrong, and try to gain insight in terms of why a person got it right but the algorithm got it wrong.
+ get a better analysis of bias and variance 


## Avoiadable bias
2 examples; assumptions:

human performance = bayes error 

**case 1**
human = 1% error
training set error = 8% error
dev set error = 10% error
big discrepancy between  bayes error and training set error
+ focus on reducing **bias**
+ avoidable bias = 8-1 = 7%

**case 2**
human = 7.5% error
training set error = 8% error
dev set error = 10% error
very small discrepancy between  bayes error and training set error
+ focus on reducing **variance**
+ avoidable bias = 8-7.5 = 0.5%

## Understanding human level performance

Human performance should be a proxy for bayes error, so in cases where we have different accuracy levels for different groups of people, e.g. in medical imagining, untrained doctor would have 3%, then trained doctor, 2%, then a team of specilaists 1%, we should use 1% as Bayes error.

## Surpassing human-level performance

problems where ML surpasses human performance (**not natural perception tasks**)
- online advertising
- product recommendations
- how long it takes to drive from a to b
- loan approvals

what about **natural perception tasks** ?
- speech recognition
- some image recognition
- medical imagining

## Improving your model performance

Techniques to reduce your avoidable bias:
+ train bigger model (more data)
+ train longer/better optimization algorithms (ADS momentum or RMSprop, or use a better algorithm like Adam)
+ find better NN architecture/hyerparameter search (RNN, CNN)


Techniques to reduce your variance:
+ more data
+ regularization (L2 regularization or dropout or data augmentation)
+ find better NN architecture/hyerparameter search (RNN, CNN)


#### WEEK 2 ####

# Error analysis 

## Carrying out error analysis

if the model shows a high error rate then make an effort to

1) generate hypotheses on why the model is failing (e.g. mislabels dogs, big cats, blurry pitures) 

2) manually inspect a sample (e.g. 100) dev set images that were misclassified and count how many fall into different categories.


This will give you an idea on how much you're likely to improve the model performance if you decide to tackle one or more of the issues.

## Cleaning up incorrectly labeled data

DL algorithms are quite robust to random, but not systematic, errors!

it's worth inspecting if the errors on the dev set originate from incorrectly labelled data (e.g. if the dog was labelled as cat, etc.). Then, pick 100-200 misclassified dev examples to see what proportion of them actually have an incorrect label. if it makes a significant difference to your ability to evaluate algorithms on your dev set, then go ahead and spend the time to fix incorrect labels. But if it doesn't make a significant difference to your ability to use the dev set to evaluate cost buyers, then it might not be the best use of your time. 

For example, if the total error is 10% but error attributable to incorrenct label is only 0.6%, probably it's not worth pursuing it.

On the other hand, if the total error is 2% but error attributable to incorrenct label is only 0.6%, probably it's worth pursuing it.

Keep in mind that with extensive label fixing you may end up with train and dev/test sets that have slightly different distrubutions. This should not be a problem and it will covered later how to deal with suh situations.

## Build your first system quickly, then iterate

build something quick and dirty and then perform error * bias/variance analysis to learn from and iterate on the solution.

# Mismatched traing and dev/test sets 

## Training and testing on different distributions

o in this video, you've seen a couple examples of when allowing your training set data to come from a different distribution than your dev and test set allows you to have much more training data. And in these examples, it will cause your learning algorithm to perform better. 

e.g. you can use data from one distribution as train and then smaller set as dev/test sets

alternatively, you can mix up a bigger and smaller sets in train sets and then use the remaining small set for dev/test.

Now one question you might ask is, should you always use all the data you have? The answer is subtle, it is not always yes. Let's look at a counter-example in the next video.

## Bias and Variance with mismatched data distribution


| Error Type | Error Value 1 | Bias type 1 | Error Value 2 | Bias type 2 |
| ---------------- | ---------------- | ---------------- | ---------------- | ---------------- |
| Human error | 0% | Avoidable bias | | |
| Training set error | 10% | Variance | | |
| Training-dev set error | 11% | | |Data Mismatch |
| Dev set error | 12% | | 20% | |
| Test set error | 12% | Degree of overfittig to the dev set | | |

there are not many systematic ways of addressing data mismatch problems but there are a few things you can try. See next chapter.

## Addressing data mismatch

How to address the mismatch data problem? SOme ideas:

+ carry out manual error analysis and try to understand the differences between the training set and the dev/test sets (e.g. in speech recognition, dev set has more car background noise).

To avoid overfitting the test set, technically for error analysis, you should manually only look at a dev set and not at the test set

+ make training/dev data more similar, e.g. synthesise car noise and your samples (artificial data synthesis)


# Learning from multiple tasks

## Transfer learning

So to be concrete, during the first phase of training when you're training on an image recognition task, you train all of the usual parameters for the neural network, all the weights, all the layers and you have something that now learns to make image recognition predictions. Having trained that neural network, what you now do to implement transfer learning is swap in a new data set X Y, where now these are radiology images. And Y are the diagnoses you want to predict and what you do is initialize the last layers' weights. Let's call that W.L. and P.L. randomly. And now, retrain the neural network on this new data set, on the new radiology data set. You have a couple options of how you retrain neural network with radiology data. You might, if you have a small radiology dataset, you might want to just retrain the weights of the last layer, just W.L. P.L., and keep the rest of the parameters fixed. If you have enough data, you could also retrain all the layers of the rest of the neural network. And the rule of thumb is maybe if you have a small data set, then just retrain the one last layer at the output layer. Or maybe that last one or two layers. But if you have a lot of data, then maybe you can retrain all the parameters in the network. And if you retrain all the parameters in the neural network, then this initial phase of training on image recognition is sometimes called pre-training, because you're using image recognitions data to pre-initialize or really pre-train the weights of the neural network. And then if you are updating all the weights afterwards, then training on the radiology data sometimes that's called fine tuning. So you hear the words pre-training and fine tuning in a deep learning context, this is what they mean when they refer to pre-training and fine tuning weights in a transfer learning source. 

**Why is it useful?**
Learning from that, from a very large image recognition data set, might help your learning algorithm do better in radiology diagnosis. It's just learned a lot about the structure and the nature of how images look like and some of that knowledge will be useful. So having learned to recognize images, it might have learned enough about you know, just what parts of different images look like, that that knowledge about lines, dots, curves, and so on, maybe small parts of objects, that knowledge could help your radiology diagnosis network learn a bit faster or learn with less data

**when does Transfer learning make sense?** 
+ task A & B have the same input (image, audio)
+ you have a lotmore   data for A (that you learn from)
+ low level features from A can be useful in learning B

# Multi-task learning

for example, each imagine can have multiple labels: pedestrians, cars, stop signs and traffic lights.

ins such cases, mutli-task learning tends to have a better performance than training 4 separate NN.

**when does multi-task learning make sense?**

+ when training on a set of tasks that could benefit from havnf shared lower-level features
+ (usually) amount of data you have for each tasks is quite similar
+ can train a big enough NN to do well on all the tasks


# end to end learning

## What is end-to-end deep learning?

There have been some data processing systems, or learning systems that require multiple stages of processing. And what end-to-end deep learning does, is it can take all those multiple stages, and replace it usually with just a single neural network.

One of the challenges of end-to-end deep learning is that you might need a lot of data before it works well. So for example, if you're training on 3,000 hours of data to build a speech recognition system, then the traditional pipeline, the full traditional pipeline works really well. It's only when you have a very large data set, you know one to say 10,000 hours of data, anything going up to maybe 100,000 hours of data that the end-to end-approach then suddenly starts to work really well.

# Whether to use end-to-end deep learning

**pros and cons of end-to-end learning**

pros:
+ let the data speak
+ less hand designing components

cons:
+ may need a lot of data
+ excluedes potentially useful hand-designed components

the key question is: **do you have sufficient data to learn the function of the complexity needed to map from X to Y?**







